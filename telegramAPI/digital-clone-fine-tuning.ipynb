{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8765305,"sourceType":"datasetVersion","datasetId":5263720}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-23T11:54:11.383107Z","iopub.execute_input":"2024-06-23T11:54:11.383491Z","iopub.status.idle":"2024-06-23T11:54:12.771985Z","shell.execute_reply.started":"2024-06-23T11:54:11.383461Z","shell.execute_reply":"2024-06-23T11:54:12.770479Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/training-txt/formatted_training_data.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"# Installing necessary libraries","metadata":{}},{"cell_type":"code","source":"!pip install telethon\n!pip install python-decouple\n!pip install asyncio\n!pip install config\n!pip install transformers\n!pip install datasets\n!pip install tokenizers\n!pip install peft\n!pip install trl\n!pip install tqdm\n\n!pip install -q accelerate\n!pip install -q -i https://pypi.org/simple/ bitsandbytes\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T13:23:22.845423Z","iopub.execute_input":"2024-06-20T13:23:22.846020Z","iopub.status.idle":"2024-06-20T13:24:35.452745Z","shell.execute_reply.started":"2024-06-20T13:23:22.845978Z","shell.execute_reply":"2024-06-20T13:24:35.451325Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting telethon\n  Downloading Telethon-1.36.0.tar.gz (606 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.3/606.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting pyaes (from telethon)\n  Downloading pyaes-1.6.1.tar.gz (28 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: rsa in /opt/conda/lib/python3.10/site-packages (from telethon) (4.9)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa->telethon) (0.5.1)\nBuilding wheels for collected packages: telethon, pyaes\n  Building wheel for telethon (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for telethon: filename=Telethon-1.36.0-py3-none-any.whl size=680879 sha256=ed4bed01803862e091cbb892357456c026be2f1b3e79a0282e89a3dd7301be7b\n  Stored in directory: /root/.cache/pip/wheels/4f/6f/86/da2c68242ca7dd92786186731ee763b7ecac8866666ac47170\n  Building wheel for pyaes (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyaes: filename=pyaes-1.6.1-py3-none-any.whl size=26346 sha256=2c8970c7eafb5a0c4ccbcf921b9f179191a4002695dd073b7106cef805119ba3\n  Stored in directory: /root/.cache/pip/wheels/d6/84/5f/ea6aef85a93c7e1922486369874f4740a5642d261e09c59140\nSuccessfully built telethon pyaes\nInstalling collected packages: pyaes, telethon\nSuccessfully installed pyaes-1.6.1 telethon-1.36.0\nCollecting python-decouple\n  Downloading python_decouple-3.8-py3-none-any.whl.metadata (14 kB)\nDownloading python_decouple-3.8-py3-none-any.whl (9.9 kB)\nInstalling collected packages: python-decouple\nSuccessfully installed python-decouple-3.8\nCollecting asyncio\n  Downloading asyncio-3.4.3-py3-none-any.whl.metadata (1.7 kB)\nDownloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: asyncio\nSuccessfully installed asyncio-3.4.3\nCollecting config\n  Downloading config-0.5.1-py2.py3-none-any.whl.metadata (1.4 kB)\nDownloading config-0.5.1-py2.py3-none-any.whl (20 kB)\nInstalling collected packages: config\nSuccessfully installed config-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from telethon.sync import TelegramClient\nfrom telethon import TelegramClient\nimport pandas as pd\nimport json \nfrom pathlib import Path\nimport json\nimport os\nimport asyncio\n\nfrom decouple import config\nfrom loguru import logger\nfrom telethon import TelegramClient\nfrom telethon.tl.custom.dialog import Dialog\nfrom telethon.tl.functions.messages import GetHistoryRequest\nfrom telethon.tl.patched import Message\n\n#For Fine Tuning\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\n\nimport transformers\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nimport bitsandbytes as bnb\nfrom trl import SFTTrainer\nfrom transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2024-06-23T17:32:22.312450Z","iopub.execute_input":"2024-06-23T17:32:22.312952Z","iopub.status.idle":"2024-06-23T17:32:22.537444Z","shell.execute_reply.started":"2024-06-23T17:32:22.312906Z","shell.execute_reply":"2024-06-23T17:32:22.534916Z"},"trusted":true},"execution_count":30,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtelethon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msync\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TelegramClient\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtelethon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TelegramClient\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'telethon'"],"ename":"ModuleNotFoundError","evalue":"No module named 'telethon'","output_type":"error"}]},{"cell_type":"markdown","source":"# Getting all messages of my channel on Telegram","metadata":{}},{"cell_type":"code","source":"#You have to get the TELEGRAM_APP_ID and TELEGRAM_APP_HASH from telegram org\nPHONE_NUMBER = 'your number with country code'\nTELEGRAM_APP_ID = \"\"\nTELEGRAM_APP_HASH = \"\"\n\n# This is the name of the session file that will be created and stored in the working directory. Session files are used to store the state of the client, so that it can be resumed later so you dont have to login everytime you run the code\nsession_name = \"session_1\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"async def main(chat_name, limit):\n    # \"async with\" creates asynchronous context managers\n    # It is an extension of the “with” expression for use only in coroutines within asyncio programs\n    async with TelegramClient(session_name, TELEGRAM_APP_ID, TELEGRAM_APP_HASH) as client:\n        \n        # Get chat info \n        chat_info = await client.get_entity(chat_name)\n        \n        # Get all the messages, given the limit\n        # It will return the latest 5 messages if limit is 5\n        messages = await client.get_messages(entity=chat_info, limit=limit)\n        \n        # return the results in a dictionary\n        return ({\"messages\": messages, \"channel\": chat_info})","metadata":{"execution":{"iopub.status.busy":"2024-06-20T13:46:38.276898Z","iopub.execute_input":"2024-06-20T13:46:38.277472Z","iopub.status.idle":"2024-06-20T13:46:38.285259Z","shell.execute_reply.started":"2024-06-20T13:46:38.277376Z","shell.execute_reply":"2024-06-20T13:46:38.283451Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# limit=None will collect all the messages from nytimes Telegram channel (https://t.me/nytimes)\n# This open an input box and ask you to input your phone number \n#  \nchat_input = \"Muslimbek_01\"\nresults = await main(chat_name = chat_input, limit=None)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T13:46:45.921121Z","iopub.execute_input":"2024-06-20T13:46:45.921645Z","iopub.status.idle":"2024-06-20T13:46:51.142294Z","shell.execute_reply.started":"2024-06-20T13:46:45.921609Z","shell.execute_reply":"2024-06-20T13:46:51.140587Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# limit=None will collect all the messages from nytimes Telegram channel (https://t.me/nytimes)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This open an input box and ask you to input your phone number \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  \u001b[39;00m\n\u001b[1;32m      4\u001b[0m chat_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMuslimbek_01\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main(chat_name \u001b[38;5;241m=\u001b[39m chat_input, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n","Cell \u001b[0;32mIn[25], line 10\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(chat_name, limit)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(chat_name, limit):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# \"async with\" creates asynchronous context managers\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# It is an extension of the “with” expression for use only in coroutines within asyncio programs\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mTelegramClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_hash\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[1;32m     11\u001b[0m         \n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Get chat info \u001b[39;00m\n\u001b[1;32m     13\u001b[0m         chat_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mget_entity(chat_name)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# Get all the messages, given the limit\u001b[39;00m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# It will return the latest 5 messages if limit is 5\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/telethon/client/telegrambaseclient.py:311\u001b[0m, in \u001b[0;36mTelegramBaseClient.__init__\u001b[0;34m(self, session, api_id, api_hash, connection, use_ipv6, proxy, local_addr, timeout, request_retries, connection_retries, retry_delay, auto_reconnect, sequential_updates, flood_sleep_threshold, raise_last_call_error, device_model, system_version, app_version, lang_code, system_lang_code, loop, base_logger, receive_updates, catch_up, entity_cache_limit)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# ':' in session.server_address is True if it's an IPv6 address\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m session\u001b[38;5;241m.\u001b[39mserver_address \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    310\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m session\u001b[38;5;241m.\u001b[39mserver_address) \u001b[38;5;241m!=\u001b[39m use_ipv6):\n\u001b[0;32m--> 311\u001b[0m     \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_dc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDEFAULT_DC_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDEFAULT_IPV6_IP\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_ipv6\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDEFAULT_IPV4_IP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDEFAULT_PORT\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflood_sleep_threshold \u001b[38;5;241m=\u001b[39m flood_sleep_threshold\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# TODO Use AsyncClassWrapper(session)\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# ChatGetter and SenderGetter can use the in-memory _mb_entity_cache\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# to avoid network access and the need for await in session files.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# TODO Session should probably return all cached\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m#      info of entities, not just the input versions\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/telethon/sessions/sqlite.py:168\u001b[0m, in \u001b[0;36mSQLiteSession.set_dc\u001b[0;34m(self, dc_id, server_address, port)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_dc\u001b[39m(\u001b[38;5;28mself\u001b[39m, dc_id, server_address, port):\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mset_dc(dc_id, server_address, port)\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_session_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# Fetch the auth_key corresponding to this data center\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselect auth_key from sessions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/telethon/sessions/sqlite.py:194\u001b[0m, in \u001b[0;36mSQLiteSession._update_session_table\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# While we can save multiple rows into the sessions table\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# currently we only want to keep ONE as the tables don't\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# tell us which auth_key's are usable and will work. Needs\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# some more work before being able to save auth_key's for\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# multiple DCs. Probably done differently.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelete from sessions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m c\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsert or replace into sessions values (?,?,?,?,?)\u001b[39m\u001b[38;5;124m'\u001b[39m, (\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dc_id,\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_address,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeout_id\n\u001b[1;32m    201\u001b[0m ))\n\u001b[1;32m    202\u001b[0m c\u001b[38;5;241m.\u001b[39mclose()\n","\u001b[0;31mOperationalError\u001b[0m: database is locked"],"ename":"OperationalError","evalue":"database is locked","output_type":"error"}]},{"cell_type":"code","source":"results.keys()\nmessages = results['messages']\nchannel = results['channel']\n\nprint(len(messages))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T23:48:17.951586Z","iopub.execute_input":"2024-06-17T23:48:17.951978Z","iopub.status.idle":"2024-06-17T23:48:17.979090Z","shell.execute_reply.started":"2024-06-17T23:48:17.951946Z","shell.execute_reply":"2024-06-17T23:48:17.977431Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"731\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see I have posted 731 post on my telegram channel.","metadata":{}},{"cell_type":"code","source":"results[\"messages\"][0].text","metadata":{"execution":{"iopub.status.busy":"2024-06-17T23:49:18.501421Z","iopub.execute_input":"2024-06-17T23:49:18.502273Z","iopub.status.idle":"2024-06-17T23:49:18.509739Z","shell.execute_reply.started":"2024-06-17T23:49:18.502236Z","shell.execute_reply":"2024-06-17T23:49:18.508550Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'**На этом у меня все! Спасибо, что были со мной на этой неделе! Мне было очень приятно делиться своими приключениями и полезной информацией с вами. Я всегда готов помочь вам с любыми вопросами об обучении, и я настоятельно рекомендую вам участвовать в семестровом обмене! Это открывает перед вами множество возможностей.**'"},"metadata":{}}]},{"cell_type":"code","source":"msg_list = [msg.to_dict() for msg in results[\"messages\"]]","metadata":{"execution":{"iopub.status.busy":"2024-06-17T23:49:29.741142Z","iopub.execute_input":"2024-06-17T23:49:29.741586Z","iopub.status.idle":"2024-06-17T23:49:29.793645Z","shell.execute_reply.started":"2024-06-17T23:49:29.741551Z","shell.execute_reply":"2024-06-17T23:49:29.792463Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Save results \n#Path(os.path.join(\"json_data\")).mkdir(parents=True, exist_ok=True)\noutput_dir = '/kaggle/working/'\nos.makedirs(output_dir, exist_ok=True)\n\nout_path = os.path.join(output_dir, f\"{chat_input}.json\")\n\nwith open(out_path, \"w\") as f:\n    json.dump(msg_list, f, default=str, ensure_ascii=False)\n\n# Optionally, print the saved file path\nprint(f\"JSON file saved to: {out_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T23:51:25.160380Z","iopub.execute_input":"2024-06-17T23:51:25.161935Z","iopub.status.idle":"2024-06-17T23:51:25.427728Z","shell.execute_reply.started":"2024-06-17T23:51:25.161884Z","shell.execute_reply":"2024-06-17T23:51:25.426545Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"JSON file saved to: /kaggle/working/Muslimbek_01.json\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_json('/kaggle/working/Muslimbek_01.json')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T23:52:19.464090Z","iopub.execute_input":"2024-06-17T23:52:19.464514Z","iopub.status.idle":"2024-06-17T23:52:19.630775Z","shell.execute_reply.started":"2024-06-17T23:52:19.464474Z","shell.execute_reply":"2024-06-17T23:52:19.629642Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"         _   id                                         peer_id  \\\n0  Message  761  {'_': 'PeerChannel', 'channel_id': 1466204272}   \n1  Message  760  {'_': 'PeerChannel', 'channel_id': 1466204272}   \n2  Message  759  {'_': 'PeerChannel', 'channel_id': 1466204272}   \n3  Message  758  {'_': 'PeerChannel', 'channel_id': 1466204272}   \n4  Message  757  {'_': 'PeerChannel', 'channel_id': 1466204272}   \n\n                       date  \\\n0 2024-05-27 11:05:16+00:00   \n1 2024-05-27 11:05:16+00:00   \n2 2024-05-27 11:05:16+00:00   \n3 2024-05-26 13:44:15+00:00   \n4 2024-05-26 13:44:15+00:00   \n\n                                             message    out  mentioned  \\\n0  На этом у меня все! Спасибо, что были со мной ...  False      False   \n1  А в последнем посте я хочу поделиться с Вами п...  False      False   \n2  В предпоследним посте я хочу поделиться некото...  False      False   \n3  Еще одно интересное мероприятие проходило в Ма...  False      False   \n4                                                     False      False   \n\n   media_unread  silent  post  ...                  edit_date  post_author  \\\n0         False   False  True  ...  2024-05-27 11:26:11+00:00          NaN   \n1         False   False  True  ...  2024-05-27 11:26:20+00:00          NaN   \n2         False   False  True  ...  2024-05-27 11:26:26+00:00          NaN   \n3         False   False  True  ...  2024-05-26 13:44:18+00:00          NaN   \n4         False   False  True  ...  2024-05-26 13:44:18+00:00          NaN   \n\n     grouped_id                                          reactions  \\\n0           NaN  {'_': 'MessageReactions', 'results': [{'_': 'R...   \n1           NaN  {'_': 'MessageReactions', 'results': [{'_': 'R...   \n2           NaN  {'_': 'MessageReactions', 'results': [{'_': 'R...   \n3  1.373385e+16                                               None   \n4  1.373385e+16                                               None   \n\n   restriction_reason  ttl_period  quick_reply_shortcut_id  effect  factcheck  \\\n0                  []         NaN                      NaN     NaN        NaN   \n1                  []         NaN                      NaN     NaN        NaN   \n2                  []         NaN                      NaN     NaN        NaN   \n3                  []         NaN                      NaN     NaN        NaN   \n4                  []         NaN                      NaN     NaN        NaN   \n\n   action  \n0     NaN  \n1     NaN  \n2     NaN  \n3     NaN  \n4     NaN  \n\n[5 rows x 40 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_</th>\n      <th>id</th>\n      <th>peer_id</th>\n      <th>date</th>\n      <th>message</th>\n      <th>out</th>\n      <th>mentioned</th>\n      <th>media_unread</th>\n      <th>silent</th>\n      <th>post</th>\n      <th>...</th>\n      <th>edit_date</th>\n      <th>post_author</th>\n      <th>grouped_id</th>\n      <th>reactions</th>\n      <th>restriction_reason</th>\n      <th>ttl_period</th>\n      <th>quick_reply_shortcut_id</th>\n      <th>effect</th>\n      <th>factcheck</th>\n      <th>action</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Message</td>\n      <td>761</td>\n      <td>{'_': 'PeerChannel', 'channel_id': 1466204272}</td>\n      <td>2024-05-27 11:05:16+00:00</td>\n      <td>На этом у меня все! Спасибо, что были со мной ...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>2024-05-27 11:26:11+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'_': 'MessageReactions', 'results': [{'_': 'R...</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Message</td>\n      <td>760</td>\n      <td>{'_': 'PeerChannel', 'channel_id': 1466204272}</td>\n      <td>2024-05-27 11:05:16+00:00</td>\n      <td>А в последнем посте я хочу поделиться с Вами п...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>2024-05-27 11:26:20+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'_': 'MessageReactions', 'results': [{'_': 'R...</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Message</td>\n      <td>759</td>\n      <td>{'_': 'PeerChannel', 'channel_id': 1466204272}</td>\n      <td>2024-05-27 11:05:16+00:00</td>\n      <td>В предпоследним посте я хочу поделиться некото...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>2024-05-27 11:26:26+00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'_': 'MessageReactions', 'results': [{'_': 'R...</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Message</td>\n      <td>758</td>\n      <td>{'_': 'PeerChannel', 'channel_id': 1466204272}</td>\n      <td>2024-05-26 13:44:15+00:00</td>\n      <td>Еще одно интересное мероприятие проходило в Ма...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>2024-05-26 13:44:18+00:00</td>\n      <td>NaN</td>\n      <td>1.373385e+16</td>\n      <td>None</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Message</td>\n      <td>757</td>\n      <td>{'_': 'PeerChannel', 'channel_id': 1466204272}</td>\n      <td>2024-05-26 13:44:15+00:00</td>\n      <td></td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>2024-05-26 13:44:18+00:00</td>\n      <td>NaN</td>\n      <td>1.373385e+16</td>\n      <td>None</td>\n      <td>[]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 40 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-17T23:52:53.143098Z","iopub.execute_input":"2024-06-17T23:52:53.143646Z","iopub.status.idle":"2024-06-17T23:52:53.152531Z","shell.execute_reply.started":"2024-06-17T23:52:53.143606Z","shell.execute_reply":"2024-06-17T23:52:53.151290Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"(731, 40)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Explaratory analysis of data","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\n\nSTOP = stopwords.words('english')\ntext = df[\"message\"].dropna()\ntext = text[text.str.len()>0]\ntext","metadata":{"execution":{"iopub.status.busy":"2024-06-17T23:54:37.353390Z","iopub.execute_input":"2024-06-17T23:54:37.353864Z","iopub.status.idle":"2024-06-17T23:54:38.732616Z","shell.execute_reply.started":"2024-06-17T23:54:37.353830Z","shell.execute_reply":"2024-06-17T23:54:38.731362Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"0      На этом у меня все! Спасибо, что были со мной ...\n1      А в последнем посте я хочу поделиться с Вами п...\n2      В предпоследним посте я хочу поделиться некото...\n3      Еще одно интересное мероприятие проходило в Ма...\n10     Еще одно интересное мероприятие было Easter Ev...\n                             ...                        \n723    Bir necha vaqt avval 6G internetini yaratish y...\n725    Dasturchi (programmist) kim? \\n\\nDasturchi- ma...\n726    Birinchi post bilan birga savol: IT nima degan...\n727    Ulkan maqsadlar va ezgu niyatlar sababli ham u...\n728    Assalomu aleykum. Kanalimizga xush kelibsiz! \\...\nName: message, Length: 588, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\ndtm=vectorizer.fit_transform(text)\nvocab = vectorizer.vocabulary_\n\n# Top 30 words \nsum_words = dtm.sum(axis=0) # a 1x9211 matrix\nwords_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n\nwords_freq[0:30]","metadata":{"execution":{"iopub.status.busy":"2024-06-18T00:07:10.054649Z","iopub.execute_input":"2024-06-18T00:07:10.055185Z","iopub.status.idle":"2024-06-18T00:07:10.168008Z","shell.execute_reply.started":"2024-06-18T00:07:10.055148Z","shell.execute_reply":"2024-06-18T00:07:10.166196Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"[('https', 24.48308473198657),\n ('youtu', 18.47896888162343),\n ('be', 18.356252000339808),\n ('va', 18.339575140820326),\n ('youtube', 17.36641923673008),\n ('uchun', 16.22914859988505),\n ('bu', 13.78930160151143),\n ('ushbu', 12.386216344500651),\n ('com', 12.269714405922143),\n ('bo', 12.0140930573792),\n ('shorts', 11.030827381099938),\n ('ham', 10.477123616210395),\n ('stop', 10.09720928250552),\n ('javob', 9.924699282798635),\n ('ma', 8.98412767955407),\n ('muslimbek_01', 8.85436353898065),\n ('daqiqa', 8.793609938632827),\n ('video', 8.281425623020072),\n ('bilan', 8.082916177679044),\n ('haqida', 8.044206723394842),\n ('yangi', 7.859540735950448),\n ('feature', 7.338270321320264),\n ('mumkin', 7.3368685834310465),\n ('share', 7.29860783679448),\n ('sima_company', 7.1446702952484165),\n ('bir', 6.965210967175569),\n ('eng', 6.796638681830508),\n ('siz', 6.332471658262747),\n ('kanalimiz', 6.30668920651635),\n ('obuna', 6.258674117868144)]"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\n\nplt.bar(words_counts[0:5])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What are the different topics discussed?\nfrom sklearn.cluster import KMeans\n\nK = 10  # Number of clusters\nkmeans = KMeans(n_clusters=K, random_state=42)\nkmeans.fit(dtm)\n\n\norder_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names_out()\n\nfor i in range(K):\n    print(f\"Cluster {i}:\"),\n    for ind in order_centroids[i, :10]:  # Print top 10 terms per cluster\n        print(f\" {terms[ind]}\"),\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T00:07:23.270733Z","iopub.execute_input":"2024-06-18T00:07:23.271166Z","iopub.status.idle":"2024-06-18T00:07:23.681569Z","shell.execute_reply.started":"2024-06-18T00:07:23.271135Z","shell.execute_reply":"2024-06-18T00:07:23.680553Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Cluster 0:\n stop\n kasbga\n kasbni\n kashf\n kaskadli\n kastyumlar\n katalogini\n kategoriya\n katta\n kattakon\n\nCluster 1:\n shorts\n com\n youtube\n https\n feature\n share\n live\n iiwfydzzb0a\n jipeq_0ywkm\n h9jfmquu5ss\n\nCluster 2:\n daqiqa\n yoʻllash\n javob\n uchun\n oʻylash\n 15\n keep\n keladigan\n kaskadli\n kastyumlar\n\nCluster 3:\n doimo\n kanalimiz\n bilan\n salomat\n sogʻ\n tabriklayman\n va\n bayram\n kuni\n muslimbek_01\n\nCluster 4:\n berib\n video\n haqida\n oʻtdim\n ma\n lumot\n videoda\n yangi\n youtube\n kanalimizda\n\nCluster 5:\n ham\n sababli\n va\n shu\n chatgpt\n ekan\n faqatgina\n bir\n muslimbek_01\n rossiya\n\nCluster 6:\n bo\n bu\n va\n uchun\n ushbu\n ham\n mumkin\n ko\n muslimbek_01\n javob\n\nCluster 7:\n va\n dasturlash\n html\n css\n sima_company\n web\n esa\n bu\n uchun\n obuna\n\nCluster 8:\n youtu\n be\n https\n lsm9gzknnam\n s0qcbry7czs\n f44nkbtliq8\n iuzyrhzn92a\n txdwsffde\n wb1_3lzaq1q\n knkvet6iqq4\n\nCluster 9:\n youtube\n video\n unutmang\n navbatdagi\n va\n ulashishni\n davom\n doʻstlarga\n ushbu\n kanalimizda\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Getting all messages that I sent on Telegram ","metadata":{}},{"cell_type":"code","source":"def get_dialogs(limit: int | None = 100) -> list[Dialog]:\n    \"\"\"Get all dialogs from the Telegram.\"\"\"\n    dialogs: list[Dialog] = client.get_dialogs(limit=limit)\n    dialogs = [dialog for dialog in dialogs if dialog.is_user]  # remove groups or channels\n    logger.info(f\"Found {len(dialogs)} dialogs\")\n    return dialogs","metadata":{"execution":{"iopub.status.busy":"2024-06-20T13:27:14.681018Z","iopub.execute_input":"2024-06-20T13:27:14.681415Z","iopub.status.idle":"2024-06-20T13:27:14.689704Z","shell.execute_reply.started":"2024-06-20T13:27:14.681386Z","shell.execute_reply":"2024-06-20T13:27:14.688599Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def parse_messages(dialog: Dialog, limit: int = 1000) -> list[dict]:\n    \"\"\"Get all messages from the dialog.\"\"\"\n    all_messages_list = []\n    offset_id = 0\n\n    while True:\n        messages: list[Message] = client(\n            GetHistoryRequest(\n                peer=dialog,\n                offset_id=offset_id,\n                offset_date=None,\n                add_offset=0,\n                limit=limit,\n                max_id=0,\n                min_id=0,\n                hash=0,\n            )\n        ).messages\n        if not messages:\n            break\n\n        all_messages_list.extend(\n            {\n                \"date\": message.date.isoformat(),\n                \"message\": message.message,\n                \"out\": message.out,\n            }\n            for message in messages\n            # Filter audio or video content\n            if message.message and not message.is_bot\n        )\n        offset_id = offset_id = messages[-1].id\n    return all_messages_list","metadata":{"execution":{"iopub.status.busy":"2024-06-20T13:27:17.053359Z","iopub.execute_input":"2024-06-20T13:27:17.053763Z","iopub.status.idle":"2024-06-20T13:27:17.063656Z","shell.execute_reply.started":"2024-06-20T13:27:17.053731Z","shell.execute_reply":"2024-06-20T13:27:17.062301Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Overall code","metadata":{}},{"cell_type":"code","source":"def save_to_json(file_to_save: list[dict[str, str]], file_name: str) -> None:\n    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n        json.dump(file_to_save, f, ensure_ascii=False)\n\n\nasync def get_dialogs(client, limit: int | None = 100) -> list[Dialog]:\n    \"\"\"Get all dialogs from Telegram.\"\"\"\n    dialogs = await client.get_dialogs(limit=limit)\n    dialogs = [dialog for dialog in dialogs if dialog.is_user]  # Remove groups or channels\n    logger.info(f\"Found {len(dialogs)} dialogs\")\n    return dialogs\n\n\nasync def parse_messages(client, dialog: Dialog, limit: int = 1000) -> list[dict]:\n    \"\"\"Get all messages from the dialog.\"\"\"\n    all_messages_list = []\n    offset_id = 0\n\n    while True:\n        history = await client(GetHistoryRequest(\n            peer=dialog.entity,\n            offset_id=offset_id,\n            offset_date=None,\n            add_offset=0,\n            limit=limit,\n            max_id=0,\n            min_id=0,\n            hash=0,\n        ))\n        messages: list[Message] = history.messages\n        if not messages:\n            break\n\n        all_messages_list.extend(\n            {\n                \"date\": message.date.isoformat(),\n                \"message\": message.message,\n                \"out\": message.out,\n            }\n            for message in messages\n            if message.message and not message.is_bot\n        )\n        offset_id = messages[-1].id\n    return all_messages_list\n\n\nasync def main(limit: int = 1000):\n    \"\"\"Parse and save all private chat messages.\"\"\"\n    async with TelegramClient(SESSION_NAME, TELEGRAM_APP_ID, TELEGRAM_APP_HASH) as client:\n        # Start the client\n        await client.start()\n        \n        # Get all dialogs\n        dialogs = await get_dialogs(client, limit=None)\n        \n        # Create directory if it does not exist\n        os.makedirs(\"data\", exist_ok=True)\n        \n        # Iterate through each dialog and save messages\n        for dialog in dialogs:\n            all_messages_list = await parse_messages(client, dialog, limit=limit)\n            save_to_json(all_messages_list, f\"data/{dialog.id}.json\")\n            logger.success(f\"Saved {len(all_messages_list)} messages for {dialog.name or dialog.entity.username}\")\n        \n        # Disconnect the client\n        await client.disconnect()\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    # Set limit for the number of messages to collect per dialog\n    limit = 1000 \n    \n    try:\n        asyncio.run(main(limit))\n    except RuntimeError as e:\n        if \"asyncio.run() cannot be called from a running event loop\" in str(e):\n            loop = asyncio.get_running_loop()\n            loop.create_task(main(limit))\n        else:\n            raise\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T14:00:27.423895Z","iopub.execute_input":"2024-06-20T14:00:27.424506Z","iopub.status.idle":"2024-06-20T14:00:27.441760Z","shell.execute_reply.started":"2024-06-20T14:00:27.424464Z","shell.execute_reply":"2024-06-20T14:00:27.440615Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/1626331748.py:91: RuntimeWarning: coroutine 'main' was never awaited\n  loop.create_task(main(limit))\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n","output_type":"stream"}]},{"cell_type":"code","source":"df_json = pd.read_json('/kaggle/input/training-txt/combined_data.json')\npd.options.display.max_colwidth = None  # disable truncation\n\ndf_json.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:09:05.550122Z","iopub.execute_input":"2024-06-23T12:09:05.551477Z","iopub.status.idle":"2024-06-23T12:09:06.694617Z","shell.execute_reply.started":"2024-06-23T12:09:05.551427Z","shell.execute_reply":"2024-06-23T12:09:06.693088Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                                                   0     \\\n0  {'date': '2022-05-28T03:43:36+00:00', 'message': 'Ok', 'out': False}   \n\n                                                                                          1     \\\n0  {'date': '2022-05-28T03:22:55+00:00', 'message': 'Bugun 22da oʻyin bor muhim', 'out': True}   \n\n                                                                   2     \\\n0  {'date': '2022-05-24T05:32:40+00:00', 'message': 'Ok', 'out': False}   \n\n                                                                                       3     \\\n0  {'date': '2022-05-24T04:02:38+00:00', 'message': 'Aktiv boʻlsangiz boʻldi', 'out': True}   \n\n                                                                                                      4     \\\n0  {'date': '2022-05-24T04:02:32+00:00', 'message': 'Óyin bóladigan kuni elon qilib qóyaman', 'out': True}   \n\n                                                                                              5     \\\n0  {'date': '2022-05-24T04:02:20+00:00', 'message': 'Online turnirlarda qatnashamiz', 'out': True}   \n\n                                                                                                                   6     \\\n0  {'date': '2022-05-24T03:29:52+00:00', 'message': 'Qanday bòladi zakovat  jamodagilar bn tanishtiring', 'out': False}   \n\n                                                                                                                                       7     \\\n0  {'date': '2022-05-24T03:27:29+00:00', 'message': 'Men ham ingliz tili òrganib chet elda magistraturani òqish niyatidaman', 'out': False}   \n\n                                                                      8     \\\n0  {'date': '2022-05-24T03:26:43+00:00', 'message': 'Zòrqu', 'out': False}   \n\n                                                                                                                     9     \\\n0  {'date': '2022-05-24T03:08:21+00:00', 'message': 'Ha Men Norvegiyada exchange student sifatida o`qiyman', 'out': True}   \n\n   ...  3724  3725  3726  3727  3728  3729  3730  3731  3732  3733  \n0  ...  None  None  None  None  None  None  None  None  None  None  \n\n[1 rows x 3734 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>3724</th>\n      <th>3725</th>\n      <th>3726</th>\n      <th>3727</th>\n      <th>3728</th>\n      <th>3729</th>\n      <th>3730</th>\n      <th>3731</th>\n      <th>3732</th>\n      <th>3733</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>{'date': '2022-05-28T03:43:36+00:00', 'message': 'Ok', 'out': False}</td>\n      <td>{'date': '2022-05-28T03:22:55+00:00', 'message': 'Bugun 22da oʻyin bor muhim', 'out': True}</td>\n      <td>{'date': '2022-05-24T05:32:40+00:00', 'message': 'Ok', 'out': False}</td>\n      <td>{'date': '2022-05-24T04:02:38+00:00', 'message': 'Aktiv boʻlsangiz boʻldi', 'out': True}</td>\n      <td>{'date': '2022-05-24T04:02:32+00:00', 'message': 'Óyin bóladigan kuni elon qilib qóyaman', 'out': True}</td>\n      <td>{'date': '2022-05-24T04:02:20+00:00', 'message': 'Online turnirlarda qatnashamiz', 'out': True}</td>\n      <td>{'date': '2022-05-24T03:29:52+00:00', 'message': 'Qanday bòladi zakovat  jamodagilar bn tanishtiring', 'out': False}</td>\n      <td>{'date': '2022-05-24T03:27:29+00:00', 'message': 'Men ham ingliz tili òrganib chet elda magistraturani òqish niyatidaman', 'out': False}</td>\n      <td>{'date': '2022-05-24T03:26:43+00:00', 'message': 'Zòrqu', 'out': False}</td>\n      <td>{'date': '2022-05-24T03:08:21+00:00', 'message': 'Ha Men Norvegiyada exchange student sifatida o`qiyman', 'out': True}</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 3734 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine tuning model","metadata":{}},{"cell_type":"markdown","source":"**I wanted to use small models as I had small number of time**","metadata":{}},{"cell_type":"code","source":"tinyllama = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n\ntokenizer = AutoTokenizer.from_pretrained(tinyllama)\nmodel = AutoModelForCausalLM.from_pretrained(tinyllama)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T22:58:12.120448Z","iopub.execute_input":"2024-06-22T22:58:12.121150Z","iopub.status.idle":"2024-06-22T22:58:27.357454Z","shell.execute_reply.started":"2024-06-22T22:58:12.121117Z","shell.execute_reply":"2024-06-22T22:58:27.356441Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d10a8ee3d4048b8a0e023dd459537b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c13f13baf584465480eea05b4b123e47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"546f018b58cd473694d0f178ed945eda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b5399b4595f41b4a199bbc63f107d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d5b36955c8f4ddaa8b29bebc311f5ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78e3d2471eef4cf3bca8baa95221ef59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9134d8a029e34e13bae0e25a2f819045"}},"metadata":{}}]},{"cell_type":"markdown","source":"**But in the end I decided to use Gpt2 as I have 3 languages Russian, English, Uzbek that I sent messages and this model is universal for these languages.**","metadata":{}},{"cell_type":"code","source":"import os\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments\n\ndef fine_tune_gpt2():\n    # Load pre-trained model and tokenizer\n    model_name = \"gpt2\"\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # Load dataset\n    def load_dataset(file_path, tokenizer, block_size=128):\n        cache_dir = '/kaggle/working/cache'\n        os.makedirs(cache_dir, exist_ok=True)\n        \n        dataset = TextDataset(\n            tokenizer=tokenizer,\n            file_path=file_path,\n            block_size=block_size,\n            cache_dir=\"/kaggle/working'\"\n        )\n        return dataset\n\n    file_path = \"/kaggle/input/training-txt/formatted_training_data.txt\"\n    dataset = load_dataset(file_path, tokenizer)\n\n    # Data collator\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n    )\n\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=\"/kaggle/working/results\",\n        overwrite_output_dir=True,\n        num_train_epochs=3,\n        per_device_train_batch_size=1, \n        gradient_accumulation_steps=4, \n        save_steps=10_000,\n        save_total_limit=2,\n        logging_dir='/kaggle/working/logs',\n        dataloader_num_workers=4, \n        gradient_checkpointing=True, \n    )\n\n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=dataset,\n    )\n\n    trainer.train()\n\n    # Save the model\n    trainer.save_model(\"/kaggle/working/fine_tuned_model\")\n    tokenizer.save_pretrained(\"/kaggle/working/fine_tuned_model\")\n\nfine_tune_gpt2()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T12:24:27.017091Z","iopub.execute_input":"2024-06-23T12:24:27.017552Z","iopub.status.idle":"2024-06-23T17:32:22.308059Z","shell.execute_reply.started":"2024-06-23T12:24:27.017520Z","shell.execute_reply":"2024-06-23T17:32:22.305785Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240623_122456-d86esxno</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/abdurahimov-muslimbek/huggingface/runs/d86esxno' target=\"_blank\">/kaggle/working/results</a></strong> to <a href='https://wandb.ai/abdurahimov-muslimbek/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/abdurahimov-muslimbek/huggingface' target=\"_blank\">https://wandb.ai/abdurahimov-muslimbek/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/abdurahimov-muslimbek/huggingface/runs/d86esxno' target=\"_blank\">https://wandb.ai/abdurahimov-muslimbek/huggingface/runs/d86esxno</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2295' max='2295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2295/2295 5:06:55, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>3.039500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.580500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.373900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.258600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ndef generate_response(prompt, model, tokenizer, max_length=100):\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Load fine-tuned model\nmodel = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_model\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_model\")\n\n# Example usage\nprompt = \"Muslimbek\"\nresponse = generate_response(prompt, model, tokenizer)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T17:35:16.874708Z","iopub.execute_input":"2024-06-23T17:35:16.875156Z","iopub.status.idle":"2024-06-23T17:35:22.419783Z","shell.execute_reply.started":"2024-06-23T17:35:16.875121Z","shell.execute_reply":"2024-06-23T17:35:22.418394Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Muslimbek \n Ha\nHa ertaga ikki izga qilishni\nYaxshi icha   yaxshimisiz? \n\n\n📌 Turnir qilib qoʻyichib qolib ketdi\nTurnir ichi  kunlari  kelganimiz\nYo‘q  yo’q,  qiziqarli  boʼladi.  Yo\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Uploading to HugginFace","metadata":{}},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-06-23T17:40:28.527715Z","iopub.execute_input":"2024-06-23T17:40:28.528288Z","iopub.status.idle":"2024-06-23T17:40:45.327420Z","shell.execute_reply.started":"2024-06-23T17:40:28.528251Z","shell.execute_reply":"2024-06-23T17:40:45.325560Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T17:47:44.781246Z","iopub.execute_input":"2024-06-23T17:47:44.781806Z","iopub.status.idle":"2024-06-23T17:47:44.816620Z","shell.execute_reply.started":"2024-06-23T17:47:44.781770Z","shell.execute_reply":"2024-06-23T17:47:44.815010Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7cc907dfbea45a4ac8bbecf7409be48"}},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\napi = HfApi()\n\nrepo_id = \"MrSimple07/my-gpt2-model-for-clone\"\n\napi.create_repo(repo_id, private=False)  \n\napi.upload_folder(\n    folder_path=\"./fine_tuned_model\",\n    repo_id=repo_id,\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T17:49:55.153912Z","iopub.execute_input":"2024-06-23T17:49:55.154384Z","iopub.status.idle":"2024-06-23T17:50:13.246744Z","shell.execute_reply.started":"2024-06-23T17:49:55.154350Z","shell.execute_reply":"2024-06-23T17:50:13.245399Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e07469d4b76c4cee8b9ba89d65163378"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632052521eef4c648da47b5d359809a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"037a27cc68d0480eb8195f3e33f2da47"}},"metadata":{}},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/MrSimple07/my-gpt2-model-for-clone/commit/9c61c5fe760269d567bc5a7f3bd9ef90ac6d59c9', commit_message='Upload folder using huggingface_hub', commit_description='', oid='9c61c5fe760269d567bc5a7f3bd9ef90ac6d59c9', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Streamlit","metadata":{}},{"cell_type":"code","source":"!pip install streamlit pyngrok transformers torch","metadata":{"execution":{"iopub.status.busy":"2024-06-23T17:58:23.029778Z","iopub.execute_input":"2024-06-23T17:58:23.030813Z","iopub.status.idle":"2024-06-23T17:58:39.393106Z","shell.execute_reply.started":"2024-06-23T17:58:23.030766Z","shell.execute_reply":"2024-06-23T17:58:39.391303Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import streamlit as st\nfrom pyngrok import ngrok\nimport nest_asyncio\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Apply nest_asyncio to allow asyncio to work in Jupyter\nnest_asyncio.apply()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T17:59:23.526423Z","iopub.execute_input":"2024-06-23T17:59:23.527008Z","iopub.status.idle":"2024-06-23T17:59:23.554504Z","shell.execute_reply.started":"2024-06-23T17:59:23.526962Z","shell.execute_reply":"2024-06-23T17:59:23.552094Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"@st.cache_resource\ndef load_model():\n    model = GPT2LMHeadModel.from_pretrained(\"MrSimple07/my-gpt2-model-for-clone\")\n    tokenizer = GPT2Tokenizer.from_pretrained(\"MrSimple07/my-gpt2-model-for-clone\")\n    return model, tokenizer\n\nmodel, tokenizer = load_model()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:00:31.757980Z","iopub.execute_input":"2024-06-23T18:00:31.758741Z","iopub.status.idle":"2024-06-23T18:00:42.418006Z","shell.execute_reply.started":"2024-06-23T18:00:31.758664Z","shell.execute_reply":"2024-06-23T18:00:42.416445Z"},"trusted":true},"execution_count":60,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/907 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f46d3b7cae42e68920812f31f3ecb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae6e29850ef43b0acaf0ce2b93947cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66035b3a6f984e1a96b4f73d83ad08aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/525 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48b4d4dd53ff41359f1a2a8087bd7f0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933a9763730448d3b6b5753dddd15fc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc70065111764cffacc808822e498651"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/470 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ababd19dbc344898bb4f5eab97646dc"}},"metadata":{}}]},{"cell_type":"code","source":"def generate_response(prompt, max_length=100):\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\ndef streamlit_app():\n    st.title(\"Your Personalized Chatbot\")\n\n    user_input = st.text_input(\"You:\", \"\")\n    if st.button(\"Send\"):\n        if user_input:\n            response = generate_response(user_input)\n            st.text_area(\"Bot:\", value=response, height=100, max_chars=None, key=None)\n        else:\n            st.warning(\"Please enter a message.\")\n\n# Write the app content to a file\nwith open(\"streamlit_app.py\", \"w\") as f:\n    f.write(\"\"\"\nimport streamlit as st\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n@st.cache_resource\ndef load_model():\n    model = GPT2LMHeadModel.from_pretrained(\"MrSimple07/my-gpt2-model\")\n    tokenizer = GPT2Tokenizer.from_pretrained(\"MrSimple07/my-gpt2-model\")\n    return model, tokenizer\n\nmodel, tokenizer = load_model()\n\ndef generate_response(prompt, max_length=100):\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\nst.title(\"Your Personalized Chatbot\")\n\nuser_input = st.text_input(\"You:\", \"\")\nif st.button(\"Send\"):\n    if user_input:\n        response = generate_response(user_input)\n        st.text_area(\"Bot:\", value=response, height=100, max_chars=None, key=None)\n    else:\n        st.warning(\"Please enter a message.\")\n    \"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:01:26.859697Z","iopub.execute_input":"2024-06-23T18:01:26.861208Z","iopub.status.idle":"2024-06-23T18:01:26.875807Z","shell.execute_reply.started":"2024-06-23T18:01:26.861155Z","shell.execute_reply":"2024-06-23T18:01:26.874781Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"get_ipython().system('streamlit run streamlit_app.py')","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:04:53.312734Z","iopub.execute_input":"2024-06-23T18:04:53.314353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!streamlit run streamlit_app.py &>/dev/null&\npublic_url = ngrok.connect(port='8501')\nprint(f' * Running on {public_url}')\n\nimport time\nwhile True:\n    time.sleep(60)\n    print(\"Keeping notebook running...\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T18:02:08.960008Z","iopub.execute_input":"2024-06-23T18:02:08.961504Z","iopub.status.idle":"2024-06-23T18:02:09.124522Z","shell.execute_reply.started":"2024-06-23T18:02:08.961454Z","shell.execute_reply":"2024-06-23T18:02:09.122892Z"},"trusted":true},"execution_count":62,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstreamlit run streamlit_app.py &>/dev/null&\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m public_url \u001b[38;5;241m=\u001b[39m ngrok\u001b[38;5;241m.\u001b[39mconnect(port\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8501\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m * Running on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublic_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py:641\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# this is *far* from a rigorous test\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# We do not support backgrounding processes because we either use\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# pexpect or pipes to read from.  Users can always just call\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;66;03m# os.system() or use ip.system=ip.system_raw\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;66;03m# if they really want a background process.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# Also, protect system call from UNC paths on Windows here too\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# as is done in InteractiveShell.system_raw\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mOSError\u001b[0m: Background processes not supported."],"ename":"OSError","evalue":"Background processes not supported.","output_type":"error"}]},{"cell_type":"code","source":"!streamlit run \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\"","metadata":{"execution":{"iopub.status.busy":"2024-06-23T17:55:08.775429Z","iopub.execute_input":"2024-06-23T17:55:08.775925Z","iopub.status.idle":"2024-06-23T17:55:56.662493Z","shell.execute_reply.started":"2024-06-23T17:55:08.775885Z","shell.execute_reply":"2024-06-23T17:55:56.660468Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}